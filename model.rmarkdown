---
title: "Datathon"
format: html
editor: visual
---


## Datathon

```{r}
library(reticulate)
library(tidyverse)
library(tidymodels)
library(parsnip)
library(discrim)
library(ranger)
library(kknn)
library(naivebayes)
library(yardstick) 

exam_grade <- c("50 - 54", "55 - 59", "60 - 64", 
                "65 - 69", "70 - 75", "80 - 100")

df<- read.csv("student_survey.csv",stringsAsFactors = TRUE) |>
  mutate(result = case_when(
    maths_score %in%  exam_grade ~ "Pass",
    english_score %in% exam_grade ~ "Pass",
    TRUE ~ "Fail"  # If no condition is met, result is set to NA
  )) |>
  select(-c("maths_score","english_score"))



# Split the data into training and testing sets
set.seed(123)
df_split <- initial_split(df, prop = 0.8, strata = result) # Stratify to maintain class balance
train_data <- training(df_split)
test_data <- testing(df_split)

# Ensure the response variable is a factor
train_data$result <- as.factor(train_data$result)
test_data$result <- as.factor(test_data$result)

# Preprocessing recipe
recipe <- recipe(result ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%  # Convert categorical to dummy variables
  step_zv(all_predictors())                   # Remove zero-variance predictors

# Logistic Regression
log_reg_model <- logistic_reg() %>%
  set_engine("glm")

log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(recipe)

# Decision Trees
tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_workflow <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(recipe)

# Random Forest
rf_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(recipe)

# Support Vector Machine
svm_model <- svm_rbf() %>%
  set_engine("kernlab") %>%
  set_mode("classification");

svm_workflow <- workflow() %>%
  add_model(svm_model) %>%
  add_recipe(recipe)

# K-Nearest Neighbors
knn_model <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification");

knn_workflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(recipe);

# Naive Bayes
nb_model <- naive_Bayes() %>%
  set_engine("naivebayes") %>%
  set_mode("classification");

nb_workflow <- workflow() %>%
  add_model(nb_model) %>%
  add_recipe(recipe);

# Neural Networks
nn_model <- mlp(hidden_units = 5) %>%
  set_engine("nnet") %>%
  set_mode("classification");

nn_workflow <- workflow() %>%
  add_model(nn_model) %>%
  add_recipe(recipe);

# LDA
lda_model <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification");

lda_workflow <- workflow() %>%
  add_model(lda_model) %>%
  add_recipe(recipe);

# Multinomial Logistic Regression
multinom_model <- multinom_reg() %>%
  set_engine("nnet") %>%
  set_mode("classification");

multinom_workflow <- workflow() %>%
  add_model(multinom_model) %>%
  add_recipe(recipe);

# Fit each model
log_reg_fit <- fit(log_reg_workflow, data = train_data)
tree_fit <- fit(tree_workflow, data = train_data)
rf_fit <- fit(rf_workflow, data = train_data)
svm_fit <- fit(svm_workflow, data = train_data)
knn_fit <- fit(knn_workflow, data = train_data)
nb_fit <- fit(nb_workflow, data = train_data)
nn_fit <- fit(nn_workflow, data = train_data)
lda_fit <- fit(lda_workflow, data = train_data)
multinom_fit <- fit(multinom_workflow, data = train_data)


# Create a function to evaluate the models
evaluate_model <- function(model, test_data, response_col) {
  # Make predictions
  test_pred <- predict(model, test_data) %>%
    bind_cols(test_data)  # Combine predictions with test data
  
  # Ensure the response column is in the test data
  response_col_sym <- rlang::sym(response_col)
  
  # Calculate classification metrics
  metrics_results <- test_pred %>%
    metrics(truth = !!response_col_sym, estimate = .pred_class)  # Use the correct response column
  
  return(metrics_results)
}


# Evaluate each model
log_reg_results <- evaluate_model(log_reg_fit, test_data, response_col = "result")
tree_results <- evaluate_model(tree_fit, test_data, response_col = "result")
rf_results <- evaluate_model(rf_fit, test_data, response_col = "result")
svm_results <- evaluate_model(svm_fit, test_data, response_col = "result")
knn_results <- evaluate_model(knn_fit, test_data, response_col = "result")
nb_results <- evaluate_model(nb_fit, test_data, response_col = "result")
nn_results <- evaluate_model(nn_fit, test_data, response_col = "result")
lda_results <- evaluate_model(lda_fit, test_data, response_col = "result")
multinom_results <- evaluate_model(multinom_fit, test_data, response_col = "result")

# Compare all results
all_results <- bind_rows(
  log_reg_results %>% mutate(model = "Logistic Regression"),
  tree_results %>% mutate(model = "Decision Tree"),
  rf_results %>% mutate(model = "Random Forest"),
  svm_results %>% mutate(model = "SVM"),
  knn_results %>% mutate(model = "k-NN"),
  nb_results %>% mutate(model = "Naive Bayes"),
  nn_results %>% mutate(model = "Neural Network"),
  lda_results %>% mutate(model = "LDA"),
  multinom_results %>% mutate(model = "Multinomial Logistic Regression")
)

# Compare models based on accuracy
best_model <- all_results %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(.estimate))

# Print the best model
best_model

```



## Fetching data from python script


```{python}


```


The radius of the circle is `{python} radius`


## Data Preparation


```{r}
#|code-fold: true
#|warning: false
#|
library(reticulate)
library(tidyverse)
library(tidymodels)
library(parsnip)
library(discrim)
library(ranger)
library(kknn)
library(naivebayes)


df<- read.csv("student_survey.csv",stringsAsFactors = TRUE) |>
  mutate(result = case_when(
    maths_score %in% c("50 - 54", "55 - 59", "60 - 64", "65 - 69", "70 - 75", "80 - 100") ~ "Yes",
    english_score %in% c("50 - 54", "55 - 59", "60 - 64", "65 - 69", "70 - 75", "80 - 100") ~ "Pass",
    TRUE ~ "Fail"  # If no condition is met, result is set to NA
  ))

write.csv(df,"student_survey_data.csv")
```


## Data Preparation


```{r}
#|warning: false
#|
# Split the data into training and testing sets
set.seed(123)
df_split <- initial_split(df, prop = 0.8, strata = result) # Stratify to maintain class balance
train_data <- training(df_split)
test_data <- testing(df_split)


# Preprocessing recipe
recipe <- recipe(result ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%  # Convert categorical to dummy variables
  step_zv(all_predictors()) %>%             # Remove zero-variance predictors
  step_normalize(all_numeric_predictors())   # Normalize numerical variables

#Logistic regression

log_reg_model <- logistic_reg() %>%
  set_engine("glm")

log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(recipe)


# Decision Trees

tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_workflow <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(recipe)


# Random forest

rf_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(recipe)


# support vector machine

svm_model <- svm_rbf() %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_workflow <- workflow() %>%
  add_model(svm_model) %>%
  add_recipe(recipe)

# KNN
knn_model <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_workflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(recipe)

#NAtive bayes
nb_model <- naive_Bayes() %>%
  set_engine("naivebayes") %>%
  set_mode("classification")

nb_workflow <- workflow() %>%
  add_model(nb_model) %>%
  add_recipe(recipe)

#neural networks
nn_model <- mlp(hidden_units = 5) %>%
  set_engine("nnet") %>%
  set_mode("classification")

nn_workflow <- workflow() %>%
  add_model(nn_model) %>%
  add_recipe(recipe)

# LDA
lda_model <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

lda_workflow <- workflow() %>%
  add_model(lda_model) %>%
  add_recipe(recipe)

#Multinomial 
multinom_model <- multinom_reg() %>%
  set_engine("nnet") %>%
  set_mode("classification")

multinom_workflow <- workflow() %>%
  add_model(multinom_model) %>%
  add_recipe(recipe)




# Fit each model
log_reg_fit <- fit(log_reg_workflow, data = train_data)
tree_fit <- fit(tree_workflow, data = train_data)
rf_fit <- fit(rf_workflow, data = train_data)
svm_fit <- fit(svm_workflow, data = train_data)
gbm_fit <- fit(gbm_workflow, data = train_data)
knn_fit <- fit(knn_workflow, data = train_data)
nb_fit <- fit(nb_workflow, data = train_data)
nn_fit <- fit(nn_workflow, data = train_data)
lda_fit <- fit(lda_workflow, data = train_data)
multinom_fit <- fit(multinom_workflow, data = train_data)



# Create a function to evaluate the models
evaluate_model <- function(model, test_data) {
  test_pred <- predict(model, test_data) %>%
    bind_cols(test_data) %>%
    metrics(truth = result, estimate = .pred_class)
  
  return(test_pred)
}

# Evaluate each model
log_reg_results <- evaluate_model(log_reg_fit, test_data)
tree_results <- evaluate_model(tree_fit, test_data)
rf_results <- evaluate_model(rf_fit, test_data)
svm_results <- evaluate_model(svm_fit, test_data)
gbm_results <- evaluate_model(gbm_fit, test_data)
knn_results <- evaluate_model(knn_fit, test_data)
nb_results <- evaluate_model(nb_fit, test_data)
nn_results <- evaluate_model(nn_fit, test_data)
lda_results <- evaluate_model(lda_fit, test_data)
multinom_results <- evaluate_model(multinom_fit, test_data)

# Compare all results
all_results <- bind_rows(
  log_reg_results %>% mutate(model = "Logistic Regression"),
  tree_results %>% mutate(model = "Decision Tree"),
  rf_results %>% mutate(model = "Random Forest"),
  svm_results %>% mutate(model = "SVM"),
  gbm_results %>% mutate(model = "GBM"),
   knn_results %>% mutate(model = "k-NN"),
  nb_results %>% mutate(model = "Naive Bayes"),
  nn_results %>% mutate(model = "Neural Network"),
  lda_results %>% mutate(model = "LDA"),
  multinom_results %>% mutate(model = "Multinomial Logistic Regression")
)

all_results


# Compare models based on accuracy
best_model <- all_results %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(.estimate))

best_model


```


=======

---
title: "Datathon"
format: html
editor: visual
---


## Datathon


## Fetching data from python script


```{python}
import os

# Define the directory you want to access
directory = "C:/Users/olanike/Documents/Data Science/mfp-report/Datathon_2024"

# Change the current working directory
os.chdir(directory)

# Confirm the current working directory
print("Current Working Directory:", os.getcwd())

```


The radius of the circle is `{python} radius`


## Data Preparation


```{r}
#|code-fold: true
#|warning: false
#|
library(reticulate)
library(tidyverse)
library(tidymodels)
library(parsnip)
library(discrim)
library(ranger)
library(kknn)
library(naivebayes)


df<- read.csv("student_survey.csv",stringsAsFactors = TRUE) |>
  mutate(result = case_when(
    maths_score %in% c("50 - 54", "55 - 59", "60 - 64", "65 - 69", "70 - 75", "80 - 100") ~ "Yes",
    english_score %in% c("50 - 54", "55 - 59", "60 - 64", "65 - 69", "70 - 75", "80 - 100") ~ "Yes",
    TRUE ~ "No"  # If no condition is met, result is set to NA
  ))

write.csv(df,"student_survey_data.csv")
```


## Data Preparation


```{r}
#|warning: false
#|
# Split the data into training and testing sets
set.seed(123)
df_split <- initial_split(df, prop = 0.8, strata = result) # Stratify to maintain class balance
train_data <- training(df_split)
test_data <- testing(df_split)


# Preprocessing recipe
recipe <- recipe(result ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) %>%  # Convert categorical to dummy variables
  step_zv(all_predictors()) %>%             # Remove zero-variance predictors
  step_normalize(all_numeric_predictors())   # Normalize numerical variables

#Logistic regression

log_reg_model <- logistic_reg() %>%
  set_engine("glm")

log_reg_workflow <- workflow() %>%
  add_model(log_reg_model) %>%
  add_recipe(recipe)


# Decision Trees

tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_workflow <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(recipe)


# Random forest

rf_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(recipe)


# support vector machine

svm_model <- svm_rbf() %>%
  set_engine("kernlab") %>%
  set_mode("classification")

svm_workflow <- workflow() %>%
  add_model(svm_model) %>%
  add_recipe(recipe)

# KNN
knn_model <- nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_workflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(recipe)

#NAtive bayes
nb_model <- naive_Bayes() %>%
  set_engine("naivebayes") %>%
  set_mode("classification")

nb_workflow <- workflow() %>%
  add_model(nb_model) %>%
  add_recipe(recipe)

#neural networks
nn_model <- mlp(hidden_units = 5) %>%
  set_engine("nnet") %>%
  set_mode("classification")

nn_workflow <- workflow() %>%
  add_model(nn_model) %>%
  add_recipe(recipe)

# LDA
lda_model <- discrim_linear() %>%
  set_engine("MASS") %>%
  set_mode("classification")

lda_workflow <- workflow() %>%
  add_model(lda_model) %>%
  add_recipe(recipe)

#Multinomial 
multinom_model <- multinom_reg() %>%
  set_engine("nnet") %>%
  set_mode("classification")

multinom_workflow <- workflow() %>%
  add_model(multinom_model) %>%
  add_recipe(recipe)




# Fit each model
log_reg_fit <- fit(log_reg_workflow, data = train_data)
tree_fit <- fit(tree_workflow, data = train_data)
rf_fit <- fit(rf_workflow, data = train_data)
svm_fit <- fit(svm_workflow, data = train_data)
gbm_fit <- fit(gbm_workflow, data = train_data)
knn_fit <- fit(knn_workflow, data = train_data)
nb_fit <- fit(nb_workflow, data = train_data)
nn_fit <- fit(nn_workflow, data = train_data)
lda_fit <- fit(lda_workflow, data = train_data)
multinom_fit <- fit(multinom_workflow, data = train_data)



# Create a function to evaluate the models
evaluate_model <- function(model, test_data) {
  test_pred <- predict(model, test_data) %>%
    bind_cols(test_data) %>%
    metrics(truth = result, estimate = .pred_class)
  
  return(test_pred)
}

# Evaluate each model
log_reg_results <- evaluate_model(log_reg_fit, test_data)
tree_results <- evaluate_model(tree_fit, test_data)
rf_results <- evaluate_model(rf_fit, test_data)
svm_results <- evaluate_model(svm_fit, test_data)
gbm_results <- evaluate_model(gbm_fit, test_data)
knn_results <- evaluate_model(knn_fit, test_data)
nb_results <- evaluate_model(nb_fit, test_data)
nn_results <- evaluate_model(nn_fit, test_data)
lda_results <- evaluate_model(lda_fit, test_data)
multinom_results <- evaluate_model(multinom_fit, test_data)

# Compare all results
all_results <- bind_rows(
  log_reg_results %>% mutate(model = "Logistic Regression"),
  tree_results %>% mutate(model = "Decision Tree"),
  rf_results %>% mutate(model = "Random Forest"),
  svm_results %>% mutate(model = "SVM"),
  gbm_results %>% mutate(model = "GBM"),
   knn_results %>% mutate(model = "k-NN"),
  nb_results %>% mutate(model = "Naive Bayes"),
  nn_results %>% mutate(model = "Neural Network"),
  lda_results %>% mutate(model = "LDA"),
  multinom_results %>% mutate(model = "Multinomial Logistic Regression")
)

all_results


# Compare models based on accuracy
best_model <- all_results %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(.estimate))

best_model


```


##EDA

```{r}
library(gridExtra)
library(ggplot2)




# Identify categorical variables
categorical_vars <- df %>%
  select(where(~ is.character(.) | is.factor(.))) %>%
  names()

# Function to create bar plots
plot_bar <- function(data, var) {
  data %>%
    count({{ var }}) %>%  # Count occurrences of each category
    ggplot(aes(x = {{ var }}, y = n, fill = {{ var }})) +
    geom_bar(stat = "identity") +
    labs(title = paste("Bar Plot of", var),
         x = var,
         y = "Count") +
    theme_minimal() +
    theme(legend.position = "none")  # Remove legend for clarity
}

# Create plots for each categorical variable
plots <- map(categorical_vars, ~ plot_bar(df, .x))

# Display the plots in a grid layout
gridExtra::grid.arrange(grobs = plots, ncol = 2)
```


## other
Yes, you can use **R** to analyze projected changes in the frequency of heavy precipitation over West Africa. R provides a wide range of packages for handling climate and meteorological data, making it well-suited for this kind of analysis. Here are some steps and tools you could use:
  
  ### 1. **Data Acquisition**
  - If you are working with projected climate data, you can obtain this from sources like **CMIP6** or **CORDEX**, which provide climate projections for different regions, including West Africa.
- The **`ncdf4`** or **`raster`** package can be used to read NetCDF climate data files, which are common formats for storing such projections.

### 2. **Preprocessing and Handling Data**
- Use the **`dplyr`**, **`tidyr`**, and **`ggplot2`** packages for data manipulation and visualization.
- To calculate the frequency of heavy precipitation events, you can define thresholds (e.g., daily precipitation above a certain percentile) and use logical operations in R to filter out such events.

### 3. **Statistical Analysis**
- Use the **`extRemes`** package for extreme value analysis. This package allows you to fit statistical models to extreme weather data, which is helpful in studying heavy precipitation events.
- For trend analysis, use **`MannKendall`** test available in the **`Kendall`** package to detect trends in precipitation extremes over time.

### 4. **Visualization**
- Visualize the spatial distribution of precipitation changes using **`ggplot2`** or **`tmap`**.
- For mapping regional differences in precipitation frequency, use **`sf`** or **`sp`** for spatial data manipulation combined with **`ggplot2`** for custom visualizations.

### Example Workflow:
1. **Reading NetCDF data:**
  ```R
library(ncdf4)
library(raster)

# Load climate data
nc_data <- nc_open("path_to_your_file.nc")
precip_data <- ncvar_get(nc_data, "precipitation_variable_name")
```

2. **Filtering Heavy Precipitation Events:**
  ```R
heavy_precip_threshold <- quantile(precip_data, 0.95)
heavy_events <- precip_data[precip_data > heavy_precip_threshold]
```

3. **Trend Analysis (using Mann-Kendall Test):**
  ```R
library(Kendall)
trend_test <- MannKendall(heavy_events)
```

4. **Plotting Results:**
  ```R
library(ggplot2)
ggplot(data, aes(x = year, y = frequency)) +
  geom_line() +
  labs(title = "Projected Changes in Heavy Precipitation",
       x = "Year", y = "Frequency of Heavy Events")
```

By combining these tools, you can analyze projected changes in heavy precipitation frequency over West Africa efficiently using R.
